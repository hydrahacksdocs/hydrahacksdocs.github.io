---
title: Local LLM Hosting
data: 2025-10-24 16:25:00 -0600
categories: [homelab,ai,llm,ollama,openwebui]
tags: [ai,llm,ollama,openwebui]
description: Proxmox Virtual Machine Deployment
author: hydra
media_subpath: /assets/posts/
pin: false
---
# Synopsis
>AI and LLM (Large Language Models) are everywhere these days. From ChatGPT, Copilot to Gemini, Deepseek, and Qwen, even Claude Code to name just a few. How would you like to host your own model for added privacy and security? Even choose the right model for the specific job? Let's do just that!
{: .prompt-info }

While most of my Hacks are as generic and hardware/virtualization agnostic as I can make them, this one is going to be very different. In order to host your own local LLM you need to have some pretty specific hardware with supported GPU/APU as well as a supported [Linux distribution](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-distributions) for the AMD GPU. You can choose to host this on Windows 11 if you like, as there are installers for all the tools I show here, but I will be using Ubuntu Server 24.04. Let's get started with the Hack!

## Hardware
For the hardware, I chose to go with a mini PC by GMKtech. I went with the [GMKtech EVO-X2 AMD Ryzen AI Max+ 395](https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc?variant=64bbb08e-da87-4bed-949b-1652cd311770). You can get one directly from GMKtech or from Amazon. It does go on sale occasionally on Amazon. A unique feature of the GMKtech EVO-X2 is it allows you to adjust the amount of RAM you want to dedicate to the GPU, which is important when you want to run some bigger LLMs.

- AMD Ryzen AI Max+ 395
  - 16 Core, 32 Treads
  - 5.1Ghz
  - 16MB L2, 64MB L3 Cache
- AMD Radeon 8060S GPU (on par with RTX 4070)
  - 40-Core RDNA 3.5 architecture
- AI NPU
  - XDNA 2 architecture
  - 50 TOPS
- 128GB 8-Channel LPDDR5X RAM
  - 8000MHz
- WiFi 7
- 2.5GbE Ethernet
- USB4.0
- 2TB PCIe 4.0 SSD
  - 2x internal PCIe 4.0 ports
- 140W Peak Power Output
- HDMI, Display Ports

## Large Language Model Testing
The GMKtech EVO-X2 has been tested with some common LLMs with the following results:

| Model | Client Token Time | Avg. Token Speed |
|-------|-------------------|------------------|
| Qwen3: 235B | 0.03s | 11 tokens/sec |
| Deepseek-R1: 32B| 10.50s | 9.26 tokens/sec |
| Open AI GPT-OSS: 120B | 0.53s | 19.25 tokens/sec |
| Open AI GPT-OSS: 20B | 0.11s | 57.15 tokens/sec |
| Qwen3: 30B | 0.18S | 55.02 tokens/sec |
LLaMA 4: 109B-Q4KM | 0.02s | 13.83 tokens/sec |

| RAM Dedicated to GPU | LLM Supported |
|----------------------|---------------|
| 64GB | Deepseek-R1: 32B |
| 96GB | Deepseek-R1: 32B, GPT-OSS: 120B |
| 128GB | Deepseek-R1: 32B, Deepseek-R1: 70B, Lama 4: 109, Qwen3: 235B |

## Ubuntu/LLM Software Setup and Configuration
The following updates need to be applied after the initial install of Ubuntu. If you need help installing, I have a Hack for [Installing Ubuntu](https://hydrahacksdocs.github.io/posts/Ubuntu_Server/).

### AMD ROCm Utility & AMD GPU Drivers
These pre-requisites are for the AMD GPU and Ubuntu Server. Use the following to check your system and install the required updates.

- Check the Linux distribution and make sure it is [supported](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-distributions).

```bash
uname -m && cat /etc/*release
```

- Check the Linux Kernel version and make sure it is [supported](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-distributions) as well.

```bash
uname -srmv
```

- To use the LLM locally you need to add your user ID to the **video** and **render** groups. Add any additional users to the group that may need to use the LLM locally.

```bash
groups
sudo usermod -a -G video,render $LOGNAME
```

- If you want to ensure any new users get these groups added upon creation, update the following config files:

```bash
echo 'ADD_EXTRA_GROUPS=1' | sudo tee -a /etc/adduser.conf
echo 'EXTRA_GROUPS=video' | sudo tee -a /etc/adduser.conf
echo 'EXTRA_GROUPS=render' | sudo tee -a /etc/adduser.conf
```

- Now let's download and install the ROCm utility:

```bash
wget https://repo.radeon.com/amdgpu-install/7.0.2/ubuntu/noble/amdgpu-install_7.0.2.70002-1_all.deb
sudo apt install ./amdgpu-install_7.0.2.70002-1_all.deb
sudo apt update
sudo apt install python3-setuptools python3-wheel
sudo apt install rocm
```

- Next let's download and install the AMD GPU drivers:

```bash
wget https://repo.radeon.com/amdgpu-install/7.0.2/ubuntu/noble/amdgpu-install_7.0.2.70002-1_all.deb
sudo apt install ./amdgpu-install_7.0.2.70002-1_all.deb
sudo apt update
sudo apt install "linux-headers-$(uname -r)" "linux-modules-extra-$(uname -r)"
sudo apt install amdgpu-dkms
```

- You will need to reboot in order to apply the new Kernel modules and update user group memberships:

```bash
sudo reboot
```

- Verify that the package for ROCm and AMD GPU were installed. Check for the rocm* and hip* packages:

```bash
apt list --installed
```

- Next verify that the rocm utility is in the PATH:

```bash
sudo update-alternatives --display rocm
```

### Docker Install

Since you will be running Ollama and OpenWeb UI in docker containers you will need to install docker on your Ubuntu Server. I have a Hack to [Install Docker on Ubuntu](https://hydrahacksdocs.github.io/posts/Docker_Ubuntu/) that you can follow to get you running.

### (Optional) Install the Portainer Agent or Portainer
If you have installed Portainer already, you can install the Portainer Agent on your new Ubuntu Server, if not, you can install Portainer as a new instance on your Ubuntu Server. I have a Hack that you can follow to [install Portainer](https://hydrahacksdocs.github.io/posts/Portainer/) if you want. You can follow the instructions on your Portainer instance to add a new environment. If not, you can use the docker commands directly as listed below.

### Ollama Docker Container Install
Below I will list both the docker command line install method as well as the config for Portainer. Choose your install method, only one method is needed.

- Install with docker run: 

```bash
docker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm
```

- Portainer Stack Compose File:

```yaml
name: local_ollama
services:
    ollama:
        devices:
            - /dev/kfd
            - /dev/dri
        volumes:
            - ollama:/root/.ollama
        ports:
            - 11434:11434
        container_name: ollama
        image: ollama/ollama:rocm
volumes:
    ollama:
        external: true
        name: ollama
```

- Update the Portainer stack.env variables if you want it to be accessible outside docker:

```yaml
OLLAMA_HOST=0.0.0.0
```

- Test your new Ollama docker install and download the llama3.2 model by running the following command. (This may take some time to give you a prompt as it needs to download the llama3.2 model first):

```bash
docker exec -it ollama ollama run llama3.2
```

- Added models are available through the Ollama library and you can access it [here](https://ollama.com/library). Simply replace the name of the model in the command above to download the new model. You will have an option to download new models through OpenWeb UI later as well.

### OpenWeb UI Container Install
openWeb UI will give you a web based application page you can run that will let you interact with your new Ollama instance in a graphical way, similar to the ChatGPT or Gemini chat window. 

- Portainer Stack Compose File:

```yaml
name: openwebui
services:
    open-webui:
        volumes:
            - open-webui:/app/backend/data
        ports:
            - 8080:8080
        container_name: ollama-webui
        restart: always
        image: ghcr.io/open-webui/open-webui:main
volumes:
    open-webui:
        external: true
        name: open-webui
```

- Update the OpenWeb UI stack.env variables to point to your Ollama container:

```yaml
OLLAMA_BASE_URL=http://<your IP address>:11434
```

- Once deployed you will be able to access OpenWebUI at the IP address:8080 of the GMKtech EVO-X2 set during your install. Set your password for initial install.

- You will want to make sure the IP address:port of your Ollama install are configured correctly. From the main menu, select your user name at the bottom of the left-hand menu. Then select **Admin Panel -> Settings (top menu), Connections**. You will see an option for **Manage Ollama API Connections** with a gear icon on the far right-hand side of the window. Verify the IP address:port of your Ollama instance is set correctly. Click on the double circle arrows to verify your connection. If you run into any issues with your installation, you can refer to the [Troubleshooting Guide](https://docs.openwebui.com/troubleshooting/) at OpenWeb UI's website for additional help.

- Once you have verified your connection you should be able to return to the main page and run a test against the lama3.2 model that should be downloaded if you tested the docker install. If you did not test it, you will need to download some models that Ollama can work with.

- Return to the **Admin Panel -> Settings -> Models** page. Select the down arrow for Manage Models. On the models page, enter a name of a model from the [Ollama Library](https://ollama.com/library) page (i.e. llama3.2) then click on the pull model download arrow to download the model. Once it is finished repeat for any added models you want. Return to the main page once you are done to test your new OpenWeb UI and Ollama install. Models are able to be changed with the dropdown menu at the top of the chat window.


>Disclaimer: This Hack is intended for entertainment purposes only and is not intended to be a complete guide to every situation or need.